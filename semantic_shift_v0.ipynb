{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d847b8fc-e339-45f4-bb1f-8e4565d0c963",
   "metadata": {},
   "source": [
    "# Word2Vec and BERT Model Training and Comparison (version 0)\n",
    "\n",
    "## Objective\n",
    "This script preprocesses three corpora (BNC, FCL, Legislation), trains Word2Vec and BERT models,\n",
    "evaluates them using synonym similarity, and analses semantic shifts across corpora.\n",
    "It includes corpus pre-processing, semantic shift analysis, and three\n",
    "semantic search prototypes: Word2Vec based, and 2 BERT based protoypes (1 with keyword detection).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import spacy, time, requests, os, itertools, json, re, pickle, torch, itertools, nltk\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.matutils import unitvec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy.spatial.distance import cosine\n",
    "from statistics import mean\n",
    "from collections import Counter\n",
    "from html import unescape\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca4c7f-f41f-41b5-a6e0-c32e255a4f33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# STEP 1: Corpus preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89679866",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load spaCy Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-clean-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove XML tags using BeautifulSoup\n",
    "    clean_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Split words where lowercase is followed by uppercase\n",
    "    clean_text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', clean_text)\n",
    "\n",
    "    # Additional cleaning: normalize whitespace\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8532ac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_text_in_chunks(text, chunk_size=100_000, min_token_length=3, stoplist=None):\n",
    "    \"\"\"Function to process text in manageable chunks, with better handling of tokens.\"\"\"\n",
    "    if stoplist is None:\n",
    "        stoplist = set()\n",
    "\n",
    "    # First clean the text to fix concatenation issues\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    tokens = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunk = text[i:i + chunk_size].lower()\n",
    "        doc = nlp(chunk)\n",
    "        \n",
    "        # Add lemmatized tokens while filtering out unwanted tokens\n",
    "        chunk_tokens = []\n",
    "        for token in doc:\n",
    "            # Only include proper tokens\n",
    "            if (token.is_alpha and \n",
    "                not token.is_stop and \n",
    "                len(token.text) >= min_token_length and\n",
    "                token.lemma_.lower() not in stoplist):\n",
    "                \n",
    "                # Additional check for concatenated words that might have been missed\n",
    "                lemma = token.lemma_.lower()\n",
    "                if re.search(r'[a-z][A-Z]', lemma):\n",
    "                    # Skip concatenated words that weren't properly split\n",
    "                    continue\n",
    "                    \n",
    "                chunk_tokens.append(lemma)\n",
    "                \n",
    "        tokens.extend(chunk_tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aed98c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Stoplist of terms to filter out\n",
    "stoplist = {\n",
    "    \"listitem\", 'itemlistelement', 'breadcrumblist', \"legaldocml\", \"xml\", \n",
    "    \"pdf\", \"doc\", \"download\", \"govuk\", \"http\", \"https\", \"www\", \"html\", \n",
    "    \"paragraph\", \"section\", \"subsection\", \"appendix\",\n",
    "    \"page\", \"footnote\", \"endnote\", \"header\", \"footer\",\n",
    "    \"anor\", \"ewhc\", \"fam\", \"lawbeta\", \"nomodule\", \"frontend\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9afe4e6-dc0e-49bf-9aa7-8d3f269af9a9",
   "metadata": {},
   "source": [
    "### Find Case Law corpus (EWHC Family division)\n",
    "Call Find Case Law API, information here: https://nationalarchives.github.io/ds-find-caselaw-docs/public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb16960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading FCL corpus from API\n",
    "api_url = \"https://caselaw.nationalarchives.gov.uk/ewhc/fam/atom.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d2dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration API settings\n",
    "total_pages = 58 # Total number of pages\n",
    "per_page = 50    # Number of cases per page\n",
    "delay = 1         # Delay between requests to avoid rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e6351d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Improved FCL data collection with better error handling and preprocessing\n",
    "fcl_data = []\n",
    "for page in range(1, total_pages + 1):\n",
    "    print(f\"Fetching page {page} of {total_pages}...\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(api_url, params={'page': page, 'per_page': per_page, 'order': '-date'})\n",
    "        response.raise_for_status()  # Raise exception for HTTP errors\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"xml\")\n",
    "        for entry in soup.find_all(\"entry\"):\n",
    "            try:\n",
    "                case_link = entry.find(\"link\", {\"rel\": \"alternate\"})[\"href\"]\n",
    "                case_response = requests.get(case_link)\n",
    "                case_response.raise_for_status()\n",
    "\n",
    "                case_text = BeautifulSoup(case_response.text, \"xml\").get_text()\n",
    "                processed_text = process_text_in_chunks(case_text, stoplist=stoplist)\n",
    "                \n",
    "                # Additional validation to ensure quality\n",
    "                if processed_text and len(processed_text) > 50:  # Ensure we have enough tokens\n",
    "                    fcl_data.append(processed_text)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing case: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching page {page}: {e}\")\n",
    "\n",
    "    time.sleep(delay)  # Avoid overwhelming the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca59514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save corpus to json file\n",
    "with open(\"/Users/caitlinwilson/Scripts_Local/fcl_fam_corpus_spacy_v3.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(fcl_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d118f14",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#load again\n",
    "with open(\"/Users/caitlinwilson/Scripts_Local/fcl_fam_corpus_spacy_v3.json\", 'r', encoding='utf-8') as f:\n",
    "    fcl_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf937280-7055-41cb-8fda-13d8dd98ef01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(fcl_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f423c6b5-f9b0-4257-bbf1-d12cf5c68416",
   "metadata": {},
   "source": [
    "### British National Corpus\n",
    "Save XMl files locally from http://www.natcorp.ox.ac.uk/XMLedition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df1198-de24-4abb-80d9-b2401e3e8ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_bnc(directory, stoplist=None, verbose=False):\n",
    "    bnc_data = []\n",
    "\n",
    "    # Collect all XML file paths first\n",
    "    xml_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".xml\"):\n",
    "                xml_files.append(os.path.join(root, file))\n",
    "\n",
    "    print(f\"Found {len(xml_files)} BNC XML files. Starting processing...\\n\")\n",
    "\n",
    "    for file_path in tqdm(xml_files, desc=\"Processing BNC files\", unit=\"file\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "\n",
    "            processed_text = process_text_in_chunks(content, stoplist=stoplist)\n",
    "\n",
    "            if processed_text and len(processed_text) > 50:\n",
    "                bnc_data.append(processed_text)\n",
    "\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    print(f\"\\n Finished loading BNC. Loaded {len(bnc_data)} valid documents.\\n\")\n",
    "    return bnc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e79aa29-55ea-48bb-b9ae-55488a80ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnc_directory = \"/Users/caitlinwilson/Scripts_Local/BNC xml\"\n",
    "bnc_data = load_and_preprocess_bnc(bnc_directory, stoplist=stoplist, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d87104-94c3-471f-a439-3ef307804d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to file\n",
    "with open(\"/Users/caitlinwilson/Scripts_Local/bnc_corpus_spacy_v3.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(bnc_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00297f7-6189-40c9-ad06-f3f0bddefd1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(bnc_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262cc210-2572-4665-8eca-1cd70cefba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load again\n",
    "with open(\"/Users/caitlinwilson/Scripts_Local/bnc_corpus_spacy_v3.json\", 'r', encoding='utf-8') as f:\n",
    "    bnc_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f231a-160c-44e9-bfdd-402a638345db",
   "metadata": {},
   "source": [
    "### Legislation\n",
    "Bulk download files from research.legislation.gov.uk here I downloaded only e-published UK Public General Acts from 2000 to 2025 in Akoma Ntoso format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd402e-5b8e-4f9e-83b1-74113a30d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_legislation(directory):\n",
    "    corpus = []\n",
    "\n",
    "    # Get all .akn files directly from the specified directory\n",
    "    akn_files = [\n",
    "        os.path.join(directory, f)\n",
    "        for f in os.listdir(directory)\n",
    "        if f.endswith(\".akn\")\n",
    "    ]\n",
    "\n",
    "    print(f\"Found {len(akn_files)} AKN files in {directory}. Starting processing...\\n\")\n",
    "\n",
    "    for i, file_path in enumerate(tqdm(akn_files, desc=\"Processing files\")):\n",
    "        print(f\"\\n{i+1}/{len(akn_files)}) Processing: {file_path}\")\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            soup = BeautifulSoup(f.read(), \"xml\")\n",
    "\n",
    "            # Extract text from <p> tags\n",
    "            paragraphs = [p.get_text() for p in soup.find_all(\"p\")]\n",
    "            text = \" \".join(paragraphs)\n",
    "\n",
    "            # Process text in chunks\n",
    "            tokenized_sentences = process_text_in_chunks(text)\n",
    "            corpus.extend(tokenized_sentences)\n",
    "\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    return corpus\n",
    "\n",
    "leg_directory = \"/Users/caitlinwilson/Scripts_Local/leg_enacted_ukgpa_2000-2025\"\n",
    "leg_data = load_and_preprocess_legislation(leg_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db857c2-b47e-4f9c-9260-c51ee921b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(leg_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ad497-a9bc-4268-9ad9-166ff96761dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to file\n",
    "with open(\"/Users/caitlinwilson/Scripts_Local/leg_ukgpa_corpus_spacy.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(leg_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d25340-a3a6-43e8-9b86-d0ea57272fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/caitlinwilson/Scripts_Local/leg_ukgpa_corpus_spacy.json\", 'r') as f:\n",
    "    leg_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7e1e4a-7b54-4443-9c7d-b9a89bc47c56",
   "metadata": {},
   "source": [
    "## Corpus statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e8a6c-3c5e-4960-82d0-16b81818f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_stats(corpus):\n",
    "    # Flatten the list of lists into a single list of tokens\n",
    "    tokens = list(itertools.chain(*corpus))  # Flatten the list of lists\n",
    "    num_tokens = len(tokens)\n",
    "    num_types = len(set(tokens))\n",
    "    type_token_ratio = num_types / num_tokens if num_tokens > 0 else 0\n",
    "    freq_dist = Counter(tokens)\n",
    "    most_common_words = freq_dist.most_common(10)\n",
    "    \n",
    "    # Return stats as a dictionary\n",
    "    return {\n",
    "        \"Total Tokens\": num_tokens,\n",
    "        \"Unique Types\": num_types,\n",
    "        \"Type-Token Ratio (TTR)\": round(type_token_ratio, 4),\n",
    "        \"Most Common Words\": most_common_words\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a8ca37-dbe7-40d4-b3e0-ce3a721fbb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stats for the FCL corpus\n",
    "fcl_stats = get_corpus_stats(fcl_data)\n",
    "bnc_stats = get_corpus_stats(bnc_data)\n",
    "leg_stats = get_corpus_stats(leg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7847927-29bd-490d-962b-c41df4319f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the FCL corpus stats\n",
    "print(\"FCL Corpus Statistics:\")\n",
    "for key, value in fcl_stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f9f84d-a7d1-4b74-bcb6-904fc322f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the BNC corpus stats\n",
    "print(\"BNC Corpus Statistics:\")\n",
    "for key, value in bnc_stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70af9b3d-e32d-47a0-8afd-0252ec77fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Leg corpus stats\n",
    "print(\"Leg Corpus Statistics:\")\n",
    "for key, value in leg_stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-training-section",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# STEP 2: Word2Vec Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyperparameters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter ranges for Word2Vec models\n",
    "vector_sizes = [300, 1000] \n",
    "windows = [5, 10]\n",
    "min_counts = [5]\n",
    "sg_values = [0, 1]  # 0 for CBOW, 1 for Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store models and names\n",
    "fcl_models = []\n",
    "fcl_model_names = []\n",
    "bnc_models = []\n",
    "bnc_model_names = []\n",
    "leg_models = []\n",
    "leg_model_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c132d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder to save models\n",
    "models_folder = \"/Users/caitlinwilson/Scripts_Local/w2v_models_improved\" \n",
    "os.makedirs(models_folder, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd8e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved model training with better parameters\n",
    "def train_word2vec_models(corpus_data, corpus_name, vector_sizes, windows, min_counts, sg_values):\n",
    "    \"\"\"Train multiple Word2Vec models with different hyperparameters.\"\"\"\n",
    "    models = []\n",
    "    model_names = []\n",
    "    \n",
    "    for vector_size in vector_sizes:\n",
    "        for window in windows:\n",
    "            for min_count in min_counts:\n",
    "                for sg in sg_values:\n",
    "                    # Generate model name\n",
    "                    model_type = \"cbow\" if sg == 0 else \"sg\"\n",
    "                    model_name = f\"{corpus_name}_{model_type}_w{window}_f{min_count}_{vector_size}_v3\"\n",
    "\n",
    "                    # Train model with more epochs and better parameters\n",
    "                    model = Word2Vec(\n",
    "                        corpus_data, \n",
    "                        min_count=min_count, \n",
    "                        vector_size=vector_size, \n",
    "                        window=window, \n",
    "                        sg=sg,\n",
    "                        epochs=10,  # Increased from default 5\n",
    "                        workers=4   # Use multiple cores\n",
    "                    )\n",
    "\n",
    "                    # Save model in list\n",
    "                    models.append(model)\n",
    "                    model_names.append(model_name)\n",
    "\n",
    "                    # Save model to folder\n",
    "                    model_path = os.path.join(models_folder, f\"{model_name}.bin\")\n",
    "                    model.save(model_path)\n",
    "\n",
    "                    print(f\"Saved model: {model_name}.bin\")\n",
    "                    \n",
    "    return models, model_names\n",
    "\n",
    "# Uncomment to train models\n",
    "fcl_models, fcl_model_names = train_word2vec_models(fcl_data, \"fcl_fam\", vector_sizes, windows, min_counts, sg_values)\n",
    "bnc_models, bnc_model_names = train_word2vec_models(bnc_data, \"bnc\", vector_sizes, windows, min_counts, sg_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fcc37d-62be-48f7-befb-5f0b219cf495",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# STEP 3: finding best Word2Vec model for each corpus using similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-bert-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previsouly saved embeddings  \n",
    "with open('/Users/caitlinwilson/Scripts_Local/bnc_bert_embeddings.pkl', 'rb') as f:\n",
    "    bnc_bert_embeddings = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(bnc_bert_embeddings)} BNC word embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5be226-7364-4bd4-877c-8b23e5a5cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SimLex-999 Data\n",
    "simlex_path = \"SimLex-999.txt\"\n",
    "simlex_df = pd.read_csv(simlex_path, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709ec2ff-cc34-4273-909c-892067577bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract synonym pairs (where SimLex-999 similarity score is high)\n",
    "simlex_synonyms = {row['word1']: row['word2'] for _, row in simlex_df.iterrows() if row['SimLex999'] >= 8.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3aa1f-c281-408f-afa4-69f1945e4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also define domain specific sysnonyms if wanted\n",
    "synonyms_fcl = {\n",
    "    'contract': 'agreement',\n",
    "    'plaintiff': 'claimant',\n",
    "    'defendant': 'respondent',\n",
    "    'liable': 'responsible',\n",
    "    'damages': 'compensation',\n",
    "    'breach': 'violation',\n",
    "    'negligence': 'carelessness',\n",
    "    'fraud': 'deception',\n",
    "    'precedent': 'authority',\n",
    "    'statute': 'legislation'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04756bc3-a66a-47a0-b697-82333d4986ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation Using SimLex-999 or FCL synonyms\n",
    "def evaluate_models_with_synonyms(models, synonym_dict):\n",
    "    models_synonymity_average = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        synonymities = []\n",
    "        \n",
    "        for word1, word2 in synonym_dict.items():\n",
    "            if word1 in model.wv and word2 in model.wv:\n",
    "                similarity = 1 - cosine(model.wv[word1], model.wv[word2])\n",
    "                synonymities.append(similarity)\n",
    "                print(f\"\\tSimilarity between '{word1}' and '{word2}' in {model_name}: {similarity:.4f}\")\n",
    "        \n",
    "        avg_synonymity = mean(synonymities) if synonymities else 0\n",
    "        print(f\"Average synonym similarity for {model_name}: {avg_synonymity:.4f}\\n\")\n",
    "        models_synonymity_average[model_name] = avg_synonymity\n",
    "    \n",
    "    return models_synonymity_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcffde0-ab9b-4bc1-97fd-c495daa155d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all .bin models from folder\n",
    "fcl_models = {}\n",
    "for file in os.listdir(models_folder):\n",
    "    if file.startswith(\"fcl\") and file.endswith(\"v2.bin\"):\n",
    "        model_path = os.path.join(models_folder, file)\n",
    "        model = Word2Vec.load(model_path)\n",
    "        fcl_models[file] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc2983-d696-4d7b-9771-fc6f0af59dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcl_models = {}\n",
    "bnc_models = {}\n",
    "leg_models = {}\n",
    "\n",
    "for file in os.listdir(models_folder):\n",
    "    model_path = os.path.join(models_folder, file)\n",
    "    \n",
    "    if file.startswith(\"fcl\") and file.endswith(\"v3.bin\"):\n",
    "        fcl_models[file] = Word2Vec.load(model_path)\n",
    "    elif file.startswith(\"bnc\") and file.endswith(\"v3.bin\"):\n",
    "        bnc_models[file] = Word2Vec.load(model_path)\n",
    "    elif file.startswith(\"leg\") and file.endswith(\".bin\"):\n",
    "        leg_models[file] = Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9613aa-72ba-4d10-8428-9116c2433981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {len(fcl_models)} FCL models\")\n",
    "print(f\"Loaded {len(bnc_models)} BNC models\")\n",
    "print(f\"Loaded {len(leg_models)} LEG models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c204b8-01a4-431e-9a4e-d689e2a2a477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "scores_leg = evaluate_models_with_synonyms(leg_models, simlex_synonyms)\n",
    "scores_bnc = evaluate_models_with_synonyms(bnc_models, simlex_synonyms)\n",
    "scores_fcl = evaluate_models_with_synonyms(fcl_models, simlex_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c4905-f5b0-49d7-a510-1e790cca3a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the Best Model\n",
    "def select_best_model(models, scores):\n",
    "    if not scores:\n",
    "        raise ValueError(\"No scores were provided. Make sure you've evaluated your models.\")\n",
    "    best_model_name = max(scores, key=scores.get)\n",
    "    print(f\"Best model: {best_model_name} with score {scores[best_model_name]:.4f}\")\n",
    "    return models[best_model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e7cf1-f4e6-4cc3-8f07-5cff49dccf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_bnc_model = select_best_model(bnc_models, scores_bnc)\n",
    "best_fcl_model = select_best_model(fcl_models, scores_fcl)\n",
    "best_leg_model = select_best_model(leg_models, scores_leg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00de22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously trained models\n",
    "best_bnc_model = Word2Vec.load(\"/Users/caitlinwilson/Scripts_Local/w2v_models_improved/bnc_sg_w10_f5_300_v3.bin\")\n",
    "best_fcl_model = Word2Vec.load(\"/Users/caitlinwilson/Scripts_Local/w2v_models_improved/fcl_fam_sg_w5_f5_300_v3.bin\")\n",
    "best_leg_model = Word2Vec.load(\"/Users/caitlinwilson/Scripts_Local/w2v_models_improved/leg_ukgpa_sg_w5_f0_300_spacy.bin\")\n",
    "\n",
    "# Quick validation check\n",
    "print(\"Word2Vec Model Validation:\")\n",
    "print(\"\\nFCL model similar to 'parent':\")\n",
    "print(best_fcl_model.wv.most_similar(\"parent\", topn=10))\n",
    "\n",
    "print(\"\\nBNC model similar to 'parent':\")\n",
    "print(best_bnc_model.wv.most_similar(\"parent\", topn=10))\n",
    "\n",
    "print(\"\\nLeg model similar to 'parent':\")\n",
    "print(best_leg_model.wv.most_similar(\"parent\", topn=10))\n",
    "\n",
    "print(\"\\nSimilarity between 'court' and 'judge' in FCL:\", best_fcl_model.wv.similarity(\"court\", \"judge\"))\n",
    "print(\"Similarity between 'parent' and 'guardian' in FCL:\", best_fcl_model.wv.similarity(\"parent\", \"guardian\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be04cbb8-2647-4edb-8d6d-e479c1493ef3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# STEP 4: Fine-tuning BERT on FCL\n",
    "Finetuning done on Google collab as kept timing out on my machine -- cells copied below for record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0208165e-d8d8-4153-af15-0366965e51eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, json\n",
    "from datasets import Dataset\n",
    "from google.colab import files\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f6876-8e6c-4261-b32d-7fe1727e7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load FCL corpus\n",
    "with open(\"fcl_fam_corpus_spacy_v3.json\", \"r\") as f:\n",
    "    fcl_data = json.load(f)\n",
    "\n",
    "# ensure data prepared\n",
    "fcl_texts = [\" \".join(doc) for doc in fcl_data[:1000]]  # limit for safe training\n",
    "fcl_dataset = Dataset.from_dict({\"text\": fcl_texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374af40f-034c-4908-9483-e00665de0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "#tokenize texts\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "\n",
    "tokenized_fcl = fcl_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40df86-4d57-45ef-bf19-2436d895706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use data collector for masked language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "#set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fcl_bert_model\",\n",
    "    eval_strategy=\"no\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    max_steps=100,\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=10,\n",
    "    fp16=False,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693efe8d-5d41-4c92-971f-93e13906a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_fcl,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e439d-1416-4077-a625-4c42aadfa454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save_pretrained(\"./fcl_bert_model\")\n",
    "tokenizer.save_pretrained(\"./fcl_bert_model\")\n",
    "\n",
    "# zip and download\n",
    "shutil.make_archive(\"fcl_bert_model\", 'zip', \"./fcl_bert_model\")\n",
    "files.download(\"fcl_bert_model.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f82a04-721a-4264-973b-d6730e5b12e6",
   "metadata": {},
   "source": [
    "### now to load the embeddings we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c16a3-50f8-4a68-9e20-35cdcccb28df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_dir = \"./fcl_bert_model\"  \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "model = BertModel.from_pretrained(model_dir)\n",
    "model.eval()  # set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713279c5-4b97-4dfb-998b-35ce1ecbaae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "lemma_embeddings = defaultdict(list)\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Aggregate embeddings per lemma\n",
    "for doc in fcl_data:\n",
    "    for lemma in doc:\n",
    "        embedding = get_embedding(lemma)\n",
    "        lemma_embeddings[lemma].append(embedding)\n",
    "\n",
    "# Average across all embeddings per lemma\n",
    "fcl_word_embeddings = {\n",
    "    lemma: np.mean(embeds, axis=0)\n",
    "    for lemma, embeds in lemma_embeddings.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77acaab7-a924-454f-ade7-1b7921b09284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/Users/caitlinwilson/Scripts_Local/fcl_bert_finetuned_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(fcl_word_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a59519-4459-4511-9197-eb0baf7450b4",
   "metadata": {},
   "source": [
    "### now create BERT embeddings for BNC (generate embeddings from all-mpnet-base-v2, no finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10e08f-9f1e-4ed9-852c-b3fdf141a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a BERT model\n",
    "bert_model = SentenceTransformer('all-mpnet-base-v2')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-bert-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved approach: Create word-level embeddings instead of document-level\n",
    "def create_word_embeddings(model, vocabulary):\n",
    "    \"\"\"Create embeddings for individual words rather than entire documents.\"\"\"\n",
    "    print(f\"Creating embeddings for {len(vocabulary)} words...\")\n",
    "    word_embeddings = {}\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    batch_size = 128\n",
    "    for i in range(0, len(vocabulary), batch_size):\n",
    "        batch = vocabulary[i:i+batch_size]\n",
    "        embeddings = model.encode(batch, show_progress_bar=False)\n",
    "        \n",
    "        for word, embedding in zip(batch, embeddings):\n",
    "            word_embeddings[word] = embedding\n",
    "            \n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Processed {i}/{len(vocabulary)} words\")\n",
    "    \n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract vocabulary from corpora\n",
    "def extract_vocabulary(corpus_data, min_length=3, max_length=20):\n",
    "    vocabulary = set()\n",
    "    \n",
    "    for document in corpus_data:\n",
    "        for token in document:\n",
    "            # Apply filtering to ensure quality vocabulary\n",
    "            if (len(token) >= min_length and \n",
    "                len(token) <= max_length and\n",
    "                token.isalpha()):\n",
    "                vocabulary.add(token)\n",
    "    \n",
    "    return list(vocabulary)\n",
    "\n",
    "# Extract vocabularies\n",
    "bnc_vocabulary = extract_vocabulary(bnc_data)\n",
    "\n",
    "print(f\"BNC vocabulary size: {len(bnc_vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-bert-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word embeddings for both corpora\n",
    "\n",
    "bnc_bert_embeddings = create_word_embeddings(bert_model, bnc_vocabulary)\n",
    "\n",
    "# Save embeddings using pickle\n",
    "with open('/Users/caitlinwilson/Scripts_Local/bnc_bert_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(bnc_bert_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3da48d-ba22-4366-8e7b-e8069b85a7eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# STEP 5: Semantic Shift comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f981b0-f681-4f07-97f9-ecca84cad3c4",
   "metadata": {},
   "source": [
    "### Semantic shift calculcated with Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b47441-0ffc-4a89-97b5-d55f9e719389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_neighbors_w2v(word, model, k=10):\n",
    "    if word not in model.wv.index_to_key:\n",
    "        return []\n",
    "    return model.wv.most_similar(word, topn=k)\n",
    "    \n",
    "def create_shift_table_w2v(word_list, fcl_model, bnc_model, k=5):\n",
    "    rows = []\n",
    "\n",
    "    for word in word_list:\n",
    "        if word not in fcl_model.wv:\n",
    "            continue\n",
    "\n",
    "        fcl_neighbors = get_nearest_neighbors_w2v(word, fcl_model, k)\n",
    "        bnc_neighbors = get_nearest_neighbors_w2v(word, bnc_model, k)\n",
    "\n",
    "        fcl_display = \", \".join([f\"{w} ({s:.2f})\" for w, s in fcl_neighbors])\n",
    "        bnc_display = \", \".join([f\"{w} ({s:.2f})\" for w, s in bnc_neighbors])\n",
    "\n",
    "        rows.append({\n",
    "            \"Word\": word,\n",
    "            \"Legal Nearest Neighbors (FCL)\": fcl_display,\n",
    "            \"General Nearest Neighbors (BNC)\": bnc_display\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c3b0-2165-41bb-87f2-3a31763b6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise as dataframe\n",
    "w2v_shift_table = create_shift_table_w2v(words_list, best_fcl_model, best_bnc_model)\n",
    "print(\"\\n--- Word2Vec Semantic Shift Table ---\\n\")\n",
    "print(w2v_shift_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d6346-0bec-4c80-9fc9-b5bffb77d390",
   "metadata": {},
   "source": [
    "### Semantic shift calculated with BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a03911-3e05-4a4b-917f-14917f186399",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fcl_word_embeddings_finetuned.pkl\", \"rb\") as f:\n",
    "    fcl_embeddings = pickle.load(f)\n",
    "\n",
    "with open(\"bnc_bert_embeddings.pkl\", \"rb\") as f:\n",
    "    bnc_embeddings = pickle.load(f)\n",
    "\n",
    "shared_vocab = list(set(fcl_embeddings.keys()) & set(bnc_embeddings.keys()))\n",
    "print(f\"Shared vocabulary size: {len(shared_vocab)}\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "for word in shared_vocab:\n",
    "    vec_fcl = fcl_embeddings[word]\n",
    "    vec_bnc = bnc_embeddings[word]\n",
    "\n",
    "    cos_sim = cosine_similarity([vec_fcl], [vec_bnc])[0][0]\n",
    "    shift_score = 1 - cos_sim  # Higher = more shift\n",
    "\n",
    "    rows.append({\n",
    "        \"word\": word,\n",
    "        \"cosine_similarity\": cos_sim,\n",
    "        \"semantic_shift\": shift_score\n",
    "    })\n",
    "\n",
    "shift_df = pd.DataFrame(rows).sort_values(by=\"semantic_shift\", ascending=False)\n",
    "shift_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b16f5ee-c39d-4a62-adb7-3e643f8aa096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise as dataframe\n",
    "top_words = shift_df.head(20).set_index(\"word\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(top_words[[\"semantic_shift\"]], cmap=\"Reds\", annot=True)\n",
    "plt.title(\"Top 20 Words with Highest Semantic Shift (FCL vs BNC)\")\n",
    "plt.xlabel(\"Semantic Shift\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d195b0eb-5025-427c-88ad-4a543a52c1c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# STEP 6: Semantic Search Prototype -- did you mean?\n",
    "Functions to suggest semantic alternatives to potential search queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "word2vec-search-section",
   "metadata": {},
   "source": [
    "## Word2Vec-based Semantic Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052138c3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_keyword_w2v(sentence, model):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Lemmatize and keep content words (nouns, verbs, adjectives)\n",
    "    content_words = [token.lemma_.lower() for token in doc if token.pos_ in ['NOUN', 'VERB', 'ADJ']]\n",
    "\n",
    "    if not content_words:\n",
    "        return None\n",
    "\n",
    "    # Only keep words that are actually in the Word2Vec vocab\n",
    "    valid_words = [word for word in content_words if word in model.wv]\n",
    "\n",
    "    if not valid_words:\n",
    "        return None\n",
    "\n",
    "    # Find the most \"central\" word: highest average similarity to others\n",
    "    sims = []\n",
    "    for word in valid_words:\n",
    "        other_words = [w for w in valid_words if w != word]\n",
    "        if not other_words:\n",
    "            avg_sim = 0\n",
    "        else:\n",
    "            avg_sim = np.mean([model.wv.similarity(word, other) for other in other_words])\n",
    "        sims.append(avg_sim)\n",
    "\n",
    "    best_word = valid_words[np.argmax(sims)]\n",
    "    return best_word\n",
    "\n",
    "def suggest_search_terms_w2v(word, fcl_model, bnc_model, k=5, threshold=0.5):\n",
    "    if word not in fcl_model.wv:\n",
    "        return f\"'{word}' not found in the legal corpus. Try another term.\"\n",
    "\n",
    "    # Get nearest neighbors from FCL model with scores\n",
    "    fcl_neighbors = fcl_model.wv.most_similar(word, topn=k*5)  # pull extra, we'll filter\n",
    "\n",
    "    filtered_neighbors = []\n",
    "    for neighbor, score in fcl_neighbors:\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        if not neighbor.isalpha() or len(neighbor) < 3 or len(neighbor) > 20:\n",
    "            continue\n",
    "        if neighbor.startswith(word) or word.startswith(neighbor):\n",
    "            if abs(len(neighbor) - len(word)) <= 2:\n",
    "                continue\n",
    "        if neighbor in bnc_model.wv:\n",
    "            bnc_similarity = bnc_model.wv.similarity(word, neighbor)\n",
    "            if score > bnc_similarity or bnc_similarity < 0.4:\n",
    "                filtered_neighbors.append((neighbor, score))\n",
    "        else:\n",
    "            filtered_neighbors.append((neighbor, score))\n",
    "\n",
    "    filtered_neighbors.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_neighbors = filtered_neighbors[:k]\n",
    "\n",
    "    if not top_neighbors:\n",
    "        return f\"No distinctive legal terms found for '{word}'. Try another term.\"\n",
    "\n",
    "    result_terms = [f\"{term} ({score:.2f})\" for term, score in top_neighbors]\n",
    "    return f\"Suggested legal search terms for '{word}': {', '.join(result_terms)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd9ab4-197a-403e-85e3-9adf10b189f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_suggestions_w2v():\n",
    "    print(\"Semantic Suggestions (Word2Vec)\")\n",
    "    print(\"Type a sentence to find legal-specific related terms. Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Enter a sentence: \").strip()\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        keyword = extract_keyword_w2v(user_input, best_fcl_model)\n",
    "\n",
    "        if not keyword:\n",
    "            print(\"Couldn't extract meaningful keyword. Try rephrasing.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nDetected keyword: {keyword}\")\n",
    "        print(suggest_search_terms_w2v(keyword, best_fcl_model, best_bnc_model))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a4b0eb-b261-4668-a69e-2187fbf4a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run - type 'exit' to quit\n",
    "semantic_suggestions_w2v()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bert-search-section",
   "metadata": {},
   "source": [
    "## BERT-based Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc921b-9768-4b50-99be-646031381c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_word(sentence, model):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    content_words = [word for word, pos in tagged if pos.startswith('N') or pos.startswith('V') or pos.startswith('J')]\n",
    "    if not content_words:\n",
    "        return None\n",
    "\n",
    "    sentence_emb = model.encode([sentence])[0]\n",
    "    word_embeddings = model.encode(content_words)\n",
    "    sims = cosine_similarity([sentence_emb], word_embeddings)[0]\n",
    "    return content_words[np.argmax(sims)].lower()\n",
    "\n",
    "def get_word_nearest_neighbors(word, model, word_embeddings_dict, k=10):\n",
    "    if word not in word_embeddings_dict:\n",
    "        return []\n",
    "\n",
    "    word_embedding = model.encode([word])[0]\n",
    "    vocab = list(word_embeddings_dict.keys())\n",
    "    embeddings = np.array([word_embeddings_dict[w] for w in vocab])\n",
    "    \n",
    "    sims = cosine_similarity([word_embedding], embeddings)[0]\n",
    "    top_k_idx = np.argsort(sims)[::-1][1:k+1]  # skip self\n",
    "    return [vocab[i] for i in top_k_idx]\n",
    "\n",
    "def suggest_search_terms(user_sentence):\n",
    "    keyword = extract_key_word(user_sentence, bert_model)\n",
    "    if not keyword:\n",
    "        return \"Couldn't extract a meaningful keyword. Try rephrasing.\"\n",
    "\n",
    "    print(f\"\\nDetected keyword: '{keyword}'\")\n",
    "\n",
    "    nn_fcl = get_word_nearest_neighbors(keyword, bert_model, fcl_word_embeddings)\n",
    "    nn_bnc = get_word_nearest_neighbors(keyword, bert_model, bnc_word_embeddings)\n",
    "\n",
    "    legal_terms = set(nn_fcl) - set(nn_bnc)\n",
    "\n",
    "    if not legal_terms:\n",
    "        return f\"'{keyword}' has a shift but no clear legal alternatives. Try a broader search.\"\n",
    "\n",
    "    return f\"Suggested legal search terms for '{keyword}': {', '.join(legal_terms)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46419298-98fe-4b7e-9278-4756971bea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_suggestions_bert():\n",
    "    print(\"Semantic Search Engine (FCL vs. General English)\")\n",
    "    print(\"Type a sentence to discover legal alternatives. Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Enter a sentence: \").strip()\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        print(suggest_search_terms(user_input))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50572dd2-af9b-475b-9b94-132d74ac4077",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_suggestions_bert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c875d-bdaf-4ad9-bf8c-1ec6ba5d3718",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# STEP 7: Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c5fd7-4d47-4140-b074-9449074ee94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seamntic Shift heatmap using cosine distance for certain words\n",
    "\n",
    "# only tracking certian words - pick words to track here\n",
    "important_words = [\"child\", \"parent\", \"consent\", \"removal\", \"custody\", \n",
    "                   \"adoption\", \"knife\", \"welfare\", \"protection\", \"authority\"]\n",
    "\n",
    "# Step 1: Define a function to compute semantic shifts for selected words\n",
    "def compute_semantic_shift_selected(model1, model2, selected_words):\n",
    "    \"\"\"Compute cosine similarity between selected words in two Word2Vec models.\"\"\"\n",
    "    shift_data = []\n",
    "\n",
    "    for word in selected_words:\n",
    "        if word in model1.wv and word in model2.wv:\n",
    "            vec1 = model1.wv[word]\n",
    "            vec2 = model2.wv[word]\n",
    "\n",
    "            cos_sim = cosine_similarity([vec1], [vec2])[0][0]\n",
    "\n",
    "            shift_data.append({\n",
    "                \"word\": word,\n",
    "                \"cosine_similarity\": cos_sim,\n",
    "                \"semantic_shift\": 1 - cos_sim  # higher = more shift\n",
    "            })\n",
    "        else:\n",
    "            print(f\" Word '{word}' not found in both models â€” skipping.\")\n",
    "\n",
    "    return pd.DataFrame(shift_data)\n",
    "\n",
    "# Step 2: Calculate semantic shifts\n",
    "shift_df = compute_semantic_shift_selected(best_fcl_model, best_bnc_model, important_words)\n",
    "\n",
    "# Step 3: Create a heatmap of the semantic shifts\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Set 'word' as index for heatmap\n",
    "heatmap_data = shift_df.set_index(\"word\")[[\"semantic_shift\"]]\n",
    "\n",
    "# Plot\n",
    "sns.heatmap(\n",
    "    heatmap_data.sort_values(by=\"semantic_shift\", ascending=False), \n",
    "    cmap=\"Reds\", \n",
    "    annot=True, \n",
    "    fmt=\".2f\",\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'label': 'Semantic Shift (1 - Cosine Similarity)'}\n",
    ")\n",
    "\n",
    "plt.title(\"Semantic Shift Heatmap (Selected Words): FCL vs BNC\")\n",
    "plt.xlabel(\"Shift Measure\")\n",
    "plt.ylabel(\"Words (Most Shifted at Top)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4942ad-421e-47e8-8545-58d20781b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors_table(model1, model2, selected_words, topn=5):\n",
    "    \"\"\"Create a table showing nearest neighbors in two models for selected words.\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for word in selected_words:\n",
    "        if word in model1.wv and word in model2.wv:\n",
    "            neighbors_model1 = [w for w, _ in model1.wv.most_similar(word, topn=topn)]\n",
    "            neighbors_model2 = [w for w, _ in model2.wv.most_similar(word, topn=topn)]\n",
    "\n",
    "            rows.append({\n",
    "                \"Word\": word,\n",
    "                \"Top Neighbors in BNC\": \", \".join(neighbors_model1),\n",
    "                \"Top Neighbors in FCL\": \", \".join(neighbors_model2)\n",
    "            })\n",
    "        else:\n",
    "            print(f\" Word '{word}' not found in both models â€” skipping.\")\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# create  neighbours comparison table\n",
    "neighbors_df = get_neighbors_table(best_bnc_model, best_fcl_model, important_words, topn=5)\n",
    "\n",
    "print(neighbors_df.to_string(index=False))\n",
    "neighbors_df.to_csv(\"neighbors_comparison_bnc_fcl.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9367f5-4374-4ed1-b914-6fbaefcb7cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_word_shift_with_neighbors(word, model1, model2, model1_name=\"Model1\", model2_name=\"Model2\", topn=5):\n",
    "    \"\"\"Improved t-SNE visualization with better labels and neighbor connections.\"\"\"\n",
    "\n",
    "    words = []\n",
    "    vectors = []\n",
    "    colors = []\n",
    "    groups = [] \n",
    "\n",
    "    # collect main word and neighbours from model1\n",
    "    if word in model1.wv:\n",
    "        vectors.append(model1.wv[word])\n",
    "        colors.append('red')\n",
    "        words.append(f\"{word} ({model1_name})\")\n",
    "        groups.append(f\"{word} ({model1_name})\")\n",
    "\n",
    "        for neighbor, _ in model1.wv.most_similar(word, topn=topn):\n",
    "            vectors.append(model1.wv[neighbor])\n",
    "            colors.append('pink')\n",
    "            words.append(f\"{neighbor} ({model1_name})\")\n",
    "            groups.append(f\"{word} ({model1_name})\")\n",
    "    else:\n",
    "        print(f\" Word '{word}' not found in {model1_name}\")\n",
    "\n",
    "    # Collect main word and neighbors from model2\n",
    "    if word in model2.wv:\n",
    "        vectors.append(model2.wv[word])\n",
    "        colors.append('blue')\n",
    "        words.append(f\"{word} ({model2_name})\")\n",
    "        groups.append(f\"{word} ({model2_name})\")\n",
    "\n",
    "        for neighbor, _ in model2.wv.most_similar(word, topn=topn):\n",
    "            vectors.append(model2.wv[neighbor])\n",
    "            colors.append('lightblue')\n",
    "            words.append(f\"{neighbor} ({model2_name})\")\n",
    "            groups.append(f\"{word} ({model2_name})\")\n",
    "    else:\n",
    "        print(f\" Word '{word}' not found in {model2_name}\")\n",
    "\n",
    "    # t-SNE projection\n",
    "    n_points = len(vectors)\n",
    "    perplexity = min(5, n_points - 1)  # Always perplexity < n_samples\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "    reduced = tsne.fit_transform(np.array(vectors))\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # plot the connections\n",
    "    for idx, group in enumerate(groups):\n",
    "        if words[idx] != group:\n",
    "            center_idx = groups.index(group)\n",
    "            plt.plot(\n",
    "                [reduced[center_idx, 0], reduced[idx, 0]],\n",
    "                [reduced[center_idx, 1], reduced[idx, 1]],\n",
    "                'k-', alpha=0.2  # faint black lines\n",
    "            )\n",
    "    # plot points\n",
    "    for idx, (x, y) in enumerate(reduced):\n",
    "        if words[idx].startswith(word):\n",
    "            size = 200  # larger for main word\n",
    "        else:\n",
    "            size = 60   # smaller for neighbors\n",
    "        plt.scatter(x, y, color=colors[idx], s=size, edgecolor='k', alpha=0.9)\n",
    "\n",
    "        plt.annotate(\n",
    "            words[idx],\n",
    "            (x + 1.0, y + 1.0),  # offset a bit to the top-right\n",
    "            fontsize=9,\n",
    "            color='black',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"none\", alpha=0.8)\n",
    "        )\n",
    "\n",
    "    plt.title(f\"t-SNE: '{word}' Semantic Neighborhoods in {model1_name} and {model2_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ff5b0-6d61-4ab0-84be-fc9dc2de0876",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_word_shift_with_neighbors(\n",
    "    word=\"consent\",\n",
    "    model1=best_fcl_model,\n",
    "    model2=best_bnc_model,\n",
    "    model1_name=\"FCL\",\n",
    "    model2_name=\"BNC\",\n",
    "    topn=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f98d8-4a1e-40c1-b1e5-f7cabf2308f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multiple_words_shift(words_list, model1, model2, model1_name=\"Model1\", model2_name=\"Model2\", topn=5):\n",
    "    \"\"\"t-SNE visualization for multiple words with two colors only: blue (model1) and red (model2).\"\"\"\n",
    "\n",
    "    all_words = []\n",
    "    vectors = []\n",
    "    colors = []\n",
    "    groups = []\n",
    "\n",
    "    for word in words_list:\n",
    "        # model1: FCL (blue)\n",
    "        if word in model1.wv:\n",
    "            vectors.append(model1.wv[word])\n",
    "            colors.append('blue')\n",
    "            all_words.append(f\"{word} ({model1_name})\")\n",
    "            groups.append(f\"{word} ({model1_name})\")\n",
    "\n",
    "            for neighbor, _ in model1.wv.most_similar(word, topn=topn):\n",
    "                vectors.append(model1.wv[neighbor])\n",
    "                colors.append('lightblue')\n",
    "                all_words.append(f\"{neighbor} ({model1_name})\")\n",
    "                groups.append(f\"{word} ({model1_name})\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Word '{word}' not found in {model1_name}\")\n",
    "\n",
    "        # model2: BNC (red)\n",
    "        if word in model2.wv:\n",
    "            vectors.append(model2.wv[word])\n",
    "            colors.append('red')\n",
    "            all_words.append(f\"{word} ({model2_name})\")\n",
    "            groups.append(f\"{word} ({model2_name})\")\n",
    "\n",
    "            for neighbor, _ in model2.wv.most_similar(word, topn=topn):\n",
    "                vectors.append(model2.wv[neighbor])\n",
    "                colors.append('pink')\n",
    "                all_words.append(f\"{neighbor} ({model2_name})\")\n",
    "                groups.append(f\"{word} ({model2_name})\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Word '{word}' not found in {model2_name}\")\n",
    "\n",
    "    # t-SNE projection\n",
    "    n_points = len(vectors)\n",
    "    perplexity = min(5, n_points - 1)\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "    reduced = tsne.fit_transform(np.array(vectors))\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 12))\n",
    "\n",
    "    # Draw connecting lines\n",
    "    for idx, group in enumerate(groups):\n",
    "        if all_words[idx] != group:\n",
    "            center_idx = groups.index(group)\n",
    "            plt.plot(\n",
    "                [reduced[center_idx, 0], reduced[idx, 0]],\n",
    "                [reduced[center_idx, 1], reduced[idx, 1]],\n",
    "                'k-', alpha=0.2\n",
    "            )\n",
    "\n",
    "    # Plot the points\n",
    "    for idx, (x, y) in enumerate(reduced):\n",
    "        if any(all_words[idx].startswith(w) for w in words_list):\n",
    "            size = 200  # main word bigger\n",
    "        else:\n",
    "            size = 60   # neighbors smaller\n",
    "        plt.scatter(x, y, color=colors[idx], s=size, edgecolor='k', alpha=0.9)\n",
    "\n",
    "        plt.annotate(\n",
    "            all_words[idx],\n",
    "            (x + 1.0, y + 1.0),\n",
    "            fontsize=9,\n",
    "            color='black',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"none\", alpha=0.8)\n",
    "        )\n",
    "\n",
    "    # Add manual legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label=model1_name),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label=model2_name)\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "    plt.title(f\"t-SNE: Semantic Shifts for {words_list} in {model1_name} and {model2_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14544311-8250-4055-bd19-e2de117157a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_multiple_words_shift(\n",
    "    words_list=[\"partner\", \"consent\", \"knife\"],\n",
    "    model1=best_fcl_model,\n",
    "    model2=best_bnc_model,\n",
    "    model1_name=\"FCL\",\n",
    "    model2_name=\"BNC\",\n",
    "    topn=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (myenv_3.10)",
   "language": "python",
   "name": "myenv_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
